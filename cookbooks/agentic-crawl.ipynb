{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Agentic Web Crawler with Tavily & OpenAI\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to build an 🧠 agentic web crawler that intelligently searches and crawls for information based on user objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build an intelligent web crawler that autonomously navigates the internet to fulfill specific information needs. Our approach combines Tavily's powerful 🔍 `/Search` and 🕸️ `/Crawl` capabilities with OpenAI's LLMs to create a system that can:\n",
    "\n",
    "1. **Understand user objectives**\n",
    "2. **Discover relevant sources** - Leverage Tavily `/Search` to identify promising websites\n",
    "3. **Make strategic decisions** - Use LLMs to evaluate and select the most valuable sites to explore\n",
    "4. **Extract comprehensive content** - Deploy Tavily's `/Crawl` API to gather in-depth information\n",
    "5. **Synthesize findings** - Consolidate and analyze the collected data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# Check for environment variables or prompt for API keys\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(TAVILY_API_KEY)\n",
    "# Analyze search results with OpenAI\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define User Objective\n",
    "\n",
    "First, we'll define the objective that the agent will use to guide its search and crawling strategy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user objective\n",
    "user_objective = \"I want to learn about the Tavily API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimize_parameters.optimize import OptimizeParameters\n",
    "from optimize_parameters.schemas import (\n",
    "    TavilySearchParameters,\n",
    ")\n",
    "\n",
    "# Using OpenAI models\n",
    "optimizer = OptimizeParameters(model=\"gpt-4o-mini\", provider=\"openai\")\n",
    "params = optimizer.optimize_parameters(user_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tavily(parameters: TavilySearchParameters) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute a search with the Tavily API using the provided parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters: The parameters to use for the search\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The raw search results from the Tavily API\n",
    "    \"\"\"\n",
    "    # Convert parameters to a dictionary and directly pass to Tavily API\n",
    "    # This simplifies the conversion process\n",
    "    params_dict = parameters.model_dump(exclude_none=True)\n",
    "    params_dict[\"search_depth\"] = \"advanced\"\n",
    "\n",
    "    # Execute the search\n",
    "    return tavily_client.search(**params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Initial Search with Tavily\n",
    "\n",
    "We'll use Tavily's search API to find the most relevant websites related to the user's objective. This will serve as our starting point for determining which sites to crawl.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the initial search\n",
    "search_results = search_tavily(params)\n",
    "\n",
    "print(f\"Found {len(search_results.get('results', []))} potential sources\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick view of the webpage via Tavily's Title, URL, and Content Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in search_results.get(\"results\", []):\n",
    "    print(f\"Title: {result.get('title')}\")\n",
    "    print(f\"URL: {result.get('url')}\")\n",
    "    print(f\"Content Snippet: {result.get('content')}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Search Results with LLM\n",
    "\n",
    "Now we'll use OpenAI's `o1` reasoning model to analyze the search results and decide which sites are the most relevant to crawl. This is where the system becomes \"agentic\" - making decisions about what to explore next.\n",
    "\n",
    "We use the titles, urls, and content snippets provided by the Tavily `/Search` endpoint as context to the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_search_results(objective: str, results: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Use LLM to analyze search results and select the most relevant sites to crawl.\"\"\"\n",
    "    results_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"{i+1}. {r['title']}\\n   URL: {r['url']}\\n   Content Snippet: {r['content']}\"\n",
    "            for i, r in enumerate(results)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    User Objective: {objective}\n",
    "    \n",
    "    Search Results:\n",
    "    {results_text}\n",
    "    \n",
    "    Identify up to 2 URLs that contain the most relevant information for the user's specific objective.\n",
    "    When selecting URLs, you may trim URL paths to more general directories if those would provide broader coverage of relevant information. For example, instead of 'amazon.com/careers/engineering/ai/', choose 'amazon.com/careers/engineering/' if the user wants information about all engineering positions rather than just AI roles.\n",
    "    The URLs will later be used as input to crawl the website, so earlier you can trim the URL paths to more general directories if those would provide broader coverage of relevant information.\n",
    "\n",
    "    Format your response as a JSON object with a 'selected_sites' array containing object with 'url'.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"o1-2024-12-17\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content).get(\"selected_sites\", [])\n",
    "\n",
    "\n",
    "selected_sites = analyze_search_results(\n",
    "    user_objective, search_results.get(\"results\", [])\n",
    ")\n",
    "print(f\"Selected {len(selected_sites)} base urls for crawling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Crawl Selected Websites\n",
    "\n",
    "Now we'll crawl each of the selected websites using Tavily's crawling API, applying the custom crawling strategies determined by our agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling function\n",
    "def crawl_website(url: str) -> Dict:\n",
    "    \"\"\"Crawl a website using Tavily's API.\"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.tavily.com/crawl\",\n",
    "        headers={\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"},\n",
    "        json={\n",
    "            \"url\": url,\n",
    "            \"limit\": 50,\n",
    "            \"max_depth\": 2,\n",
    "            \"extract_depth\": \"advanced\",\n",
    "            \"max_breadth\": 50,\n",
    "            \"select_domains\": [],\n",
    "            # \"select_paths\": [\"/examples/*\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error crawling {url}: {response.status_code} {response.text}\")\n",
    "        return {\"url\": url, \"status\": \"error\", \"results\": []}\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform crawling\n",
    "crawl_results = [crawl_website(site[\"url\"]) for site in selected_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_fields(crawl_results):\n",
    "    data_fields = []\n",
    "    for result in crawl_results:\n",
    "        # Check if this is a successful crawl with data\n",
    "        if result.get(\"success\") and result.get(\"results\"):\n",
    "            data_fields.extend(result[\"results\"])\n",
    "    return data_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_data = extract_data_fields(crawl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process and Extract Key Information\n",
    "\n",
    "Now we'll process the crawled content to extract the most relevant information for the user's objective. We'll organize this information by source and relevance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have your crawl_result\n",
    "\n",
    "\n",
    "def generate_batch_reports(crawl_result, batch_size=5):\n",
    "    \"\"\"\n",
    "    Process crawl results in batches and generate reports using an LLM.\n",
    "\n",
    "    Args:\n",
    "        crawl_result: The results from your web crawl\n",
    "        batch_size: Number of results to analyze in each LLM call\n",
    "\n",
    "    Returns:\n",
    "        A list of batch reports and a combined final report\n",
    "    \"\"\"\n",
    "    # Split crawl results into batches of 5\n",
    "    batches = []\n",
    "    for i in range(0, len(crawl_result), batch_size):\n",
    "        batch = crawl_result[i : i + batch_size]\n",
    "        batches.append(batch)\n",
    "\n",
    "    print(f\"Split crawl results into {len(batches)} batches\")\n",
    "\n",
    "    # Process each batch with the LLM\n",
    "    batch_reports = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"Processing batch {i+1}/{len(batches)}...\")\n",
    "\n",
    "        # Create a prompt for the LLM to analyze this batch\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following web crawl results and provide a detailed summary of the key information:\n",
    "        \n",
    "        {batch}\n",
    "                \n",
    "        Format your response as a well-structured report section.\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the LLM with the prompt\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        batch_reports.append(response)\n",
    "\n",
    "    # Combine the batch reports into a final comprehensive report\n",
    "    prompt = f\"\"\"\n",
    "    User Objective: {user_objective}\n",
    "    Create a report based on the findings:\n",
    "    {batch_reports}\n",
    "    \"\"\"\n",
    "\n",
    "    final_report = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return final_report.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function on your crawl results\n",
    "report_results = generate_batch_reports(crawl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save the report to a file\n",
    "with open(\"report.md\", \"w\") as f:\n",
    "    f.write(report_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the saved report file: [report.md](report.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL REPORT:\")\n",
    "print(report_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

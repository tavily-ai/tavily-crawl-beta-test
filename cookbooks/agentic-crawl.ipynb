{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Agentic Web Crawler with Tavily & OpenAI\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to build an agentic web crawler that intelligently searches and crawls for information based on user objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build an intelligent web crawler that autonomously navigates the internet to fulfill specific information needs. Our approach combines Tavily's powerful 🔍 `/Search` and 🕸️ `/Crawl` capabilities with OpenAI's LLMs to create a system that can:\n",
    "\n",
    "1. **Understand user objectives**\n",
    "2. **Discover relevant sources** - Leverage Tavily `/Search` to identify promising websites\n",
    "3. **Make strategic decisions** - Use LLMs to evaluate and select the most valuable sites to explore\n",
    "4. **Extract comprehensive content** - Deploy Tavily's `/Crawl` API to gather in-depth information\n",
    "5. **Synthesize findings** - Consolidate and analyze the collected data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# Check for environment variables or prompt for API keys\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(TAVILY_API_KEY)\n",
    "# Analyze search results with OpenAI\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define User Objective\n",
    "\n",
    "First, we'll define the objective that the agent will use to guide its search and crawling strategy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user objective\n",
    "user_objective = \"I want to learn about the Tavily API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimize_parameters.optimize import OptimizeParameters\n",
    "from optimize_parameters.schemas import (\n",
    "    TavilySearchParameters,\n",
    ")\n",
    "\n",
    "# Using OpenAI models\n",
    "optimizer = OptimizeParameters(model=\"gpt-4o-mini\", provider=\"openai\")\n",
    "params = optimizer.optimize_parameters(user_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TavilySearchParameters(query='Tavily API', include_domains=None, exclude_domains=None, include_images=False, include_image_descriptions=False, topic='general', time_range=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tavily(parameters: TavilySearchParameters) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute a search with the Tavily API using the provided parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters: The parameters to use for the search\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The raw search results from the Tavily API\n",
    "    \"\"\"\n",
    "    # Convert parameters to a dictionary and directly pass to Tavily API\n",
    "    # This simplifies the conversion process\n",
    "    params_dict = parameters.model_dump(exclude_none=True)\n",
    "    params_dict[\"search_depth\"] = \"advanced\"\n",
    "\n",
    "    # Execute the search\n",
    "    return tavily_client.search(**params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Initial Search with Tavily\n",
    "\n",
    "We'll use Tavily's search API to find the most relevant websites related to the user's objective. This will serve as our starting point for determining which sites to crawl.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 potential sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the initial search\n",
    "search_results = search_tavily(params)\n",
    "\n",
    "print(f\"Found {len(search_results.get('results', []))} potential sources\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick view of the webpage via Tavily's Title, URL, and Content Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Tavily\n",
      "URL: https://tavily.com/\n",
      "Content Snippet: Tavily Search API is a specialized search engine designed for Large Language Models (LLMs) and AI agents. It provides real-time, accurate, and unbiased information, enabling AI applications to retrieve and process data efficiently. Tavily is built with AI developers in mind, simplifying the process of integrating dynamic web information into AI-driven solutions.\n",
      "\n",
      "How is Tavily Search API different from other APIs? [...] Unlike Bing, Google and SerpAPI, Tavily Search API reviews multiple sources to find the most relevant content from each source, delivering concise, ready-to-use information optimized for LLM context. This focus on RAG and LLMs ensures your AI applications access only the highest-quality data. Tavily Search API is also more affordable and flexible.\n",
      "\n",
      "What are the key advantages of using Tavily Search API? [...] ### Tavily is trusted by AI leaders around the world\n",
      "\n",
      "![quotes](./images/icons/quotes.svg)\n",
      "\n",
      "We've been using Tavily's Search API to power Athena, our Enterprise AI Data Analyst. The ability\n",
      "to retrieve accurate, real-time information tailored for AI agents has significantly enhanced Athena’s\n",
      "research capabilities. Tavily's focus on delivering factual and explicit results aligns perfectly with\n",
      "our commitment to providing trustworthy and sourced insights to our enterprise clients.\n",
      "\n",
      "\n",
      "Title: Boost Your RAG Performance with Tavily Search API | by Minh Le Duc\n",
      "URL: https://medium.com/@minhle_0210/boost-your-rag-performance-with-tavily-search-api-607a6437ab8e\n",
      "Content Snippet: Boost Your RAG Performance with Tavily Search API | by Minh Le Duc | Medium Boost Your RAG Performance with Tavily Search API Tavily Search API is a specialized search engine designed to supercharge AI applications. Unlike traditional search engines, Tavily is optimized for L_arge Language Models (LLMs) and _Retrieval Augmented Generation (RAG) tasks. With a focus on speed, accuracy, and relevance, Tavily delivers search results tailored to AI applications. The Tavily Search API emerges as a vital tool for developers seeking to build more robust and reliable LLMs and RAG systems. With its focus on accuracy, speed, and adaptability, Tavily Search API is a game-changer for the future of AI-powered search and information retrieval.\n",
      "\n",
      "\n",
      "Title: Tavily Search API - LangChain\n",
      "URL: https://python.langchain.com/v0.1/docs/integrations/retrievers/tavily/\n",
      "Content Snippet: Tavily Search API\n",
      "\n",
      "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n",
      "\n",
      "We can use this as a retriever. It will show functionality specific to this integration. After going through, it may be useful to explore relevant use-case pages to learn how to use this vectorstore as part of a larger chain.\n",
      "\n",
      "Setup​ [...] The integration lives in the langchain-community package. We also need to install the tavily-python package itself.\n",
      "\n",
      "We also need to set our Tavily API key.\n",
      "\n",
      "It's also helpful (but not needed) to set up LangSmith for best-in-class observability\n",
      "\n",
      "Usage​\n",
      "\n",
      "API Reference:\n",
      "\n",
      "Chaining​\n",
      "\n",
      "We can easily combine this retriever in to a chain.\n",
      "\n",
      "API Reference:\n",
      "\n",
      "Help us out by providing feedback on this documentation page:\n",
      "\n",
      "\n",
      "Title: Tavily Search - LangChain\n",
      "URL: https://python.langchain.com/docs/integrations/tools/tavily_search/\n",
      "Content Snippet: Tavily Search\n",
      "\n",
      "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n",
      "\n",
      "Overview​\n",
      "\n",
      "Integration details​\n",
      "\n",
      "Class | Package | Serializable | JS support | Package latest\n",
      "TavilySearch | langchain-tavily | ✅ | ❌ | \n",
      "Tool features​\n",
      "\n",
      "Returns artifact | Native async | Return data | Pricing\n",
      "❌ | ✅ | title, URL, content snippet, raw_content, answer, images | 1,000 free searches / month\n",
      "Setup​ [...] The integration lives in the langchain-tavily package.\n",
      "\n",
      "Credentials​\n",
      "\n",
      "We also need to set our Tavily API key. You can get an API key by visiting this site and creating an account.\n",
      "\n",
      "Instantiation​\n",
      "\n",
      "Here we show how to instantiate an instance of the Tavily search tool. The tool accepts various parameters to customize the search. After instantiation we invoke the tool with a simple query. This tool allows you to complete search queries using Tavily's Search API endpoint.\n",
      "\n",
      "\n",
      "Title: Tavily AI - GitHub\n",
      "URL: https://github.com/tavily-ai\n",
      "Content Snippet: Tavily AI\n",
      "Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick and persistent search results\n",
      "\n",
      "230 followers\n",
      "United States of America\n",
      "http://tavily.com\n",
      "company/tavily\n",
      "@tavilyai\n",
      "\n",
      "support@tavily.com\n",
      "\n",
      "\n",
      "Overview\n",
      "\n",
      "Repositories\n",
      "Projects\n",
      "Packages\n",
      "People\n",
      "\n",
      "More\n",
      "\n",
      "Overview\n",
      "Repositories\n",
      "Projects\n",
      "Packages\n",
      "People\n",
      "\n",
      "Popular repositories Loading\n",
      "\n",
      "\n",
      "tavily-python tavily-python Public\n",
      "A python wrapper for Tavily search API\n",
      "Python 608 68\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in search_results.get(\"results\", []):\n",
    "    print(f\"Title: {result.get('title')}\")\n",
    "    print(f\"URL: {result.get('url')}\")\n",
    "    print(f\"Content Snippet: {result.get('content')}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Search Results with LLM\n",
    "\n",
    "Now we'll use OpenAI's `o1` reasoning model to analyze the search results and decide which sites are the most relevant to crawl. This is where the system becomes \"agentic\" - making decisions about what to explore next.\n",
    "\n",
    "We use the titles, urls, and content snippets provided by the Tavily `/Search` endpoint as context to the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 base urls for crawling.\n"
     ]
    }
   ],
   "source": [
    "def analyze_search_results(objective: str, results: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Use LLM to analyze search results and select the most relevant sites to crawl.\"\"\"\n",
    "    results_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"{i+1}. {r['title']}\\n   URL: {r['url']}\\n   Content Snippet: {r['content']}\"\n",
    "            for i, r in enumerate(results)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    User Objective: {objective}\n",
    "    \n",
    "    Search Results:\n",
    "    {results_text}\n",
    "    \n",
    "    Identify up to 2 URLs that contain the most relevant information for the user's specific objective.\n",
    "    When selecting URLs, you may trim URL paths to more general directories if those would provide broader coverage of relevant information. For example, instead of 'amazon.com/careers/engineering/ai/', choose 'amazon.com/careers/engineering/' if the user wants information about all engineering positions rather than just AI roles.\n",
    "    The URLs will later be used as input to crawl the website, so earlier you can trim the URL paths to more general directories if those would provide broader coverage of relevant information.\n",
    "\n",
    "    Format your response as a JSON object with a 'selected_sites' array containing object with 'url'.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"o1-2024-12-17\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content).get(\"selected_sites\", [])\n",
    "\n",
    "\n",
    "selected_sites = analyze_search_results(\n",
    "    user_objective, search_results.get(\"results\", [])\n",
    ")\n",
    "print(f\"Selected {len(selected_sites)} base urls for crawling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://tavily.com/'},\n",
       " {'url': 'https://python.langchain.com/v0.1/docs/integrations/retrievers/tavily/'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Crawl Selected Websites\n",
    "\n",
    "Now we'll crawl each of the selected websites using Tavily's crawling API, applying the custom crawling strategies determined by our agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling function\n",
    "def crawl_website(url: str) -> Dict:\n",
    "    \"\"\"Crawl a website using Tavily's API.\"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.tavily.com/crawl\",\n",
    "        headers={\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"},\n",
    "        json={\n",
    "            \"url\": url,\n",
    "            \"limit\": 50,\n",
    "            \"max_depth\": 2,\n",
    "            \"extract_depth\": \"basic\",\n",
    "            \"max_breadth\": 20,\n",
    "            \"select_domains\": [],\n",
    "            # \"select_paths\": [\"/examples/*\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error crawling {url}: {response.status_code} {response.text}\")\n",
    "        return {\"url\": url, \"status\": \"error\", \"results\": []}\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform crawling (this may take a minute or two)\n",
    "crawl_results = [crawl_website(site[\"url\"]) for site in selected_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_fields(crawl_results):\n",
    "    data_fields = []\n",
    "    for result in crawl_results:\n",
    "        # Check if this is a successful crawl with data\n",
    "        if result.get(\"success\") and result.get(\"results\"):\n",
    "            data_fields.extend(result[\"results\"])\n",
    "    return data_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_data = extract_data_fields(crawl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process and Extract Key Information\n",
    "\n",
    "Now we'll process the crawled content to extract the most relevant information for the user's objective. We'll organize this information by source and relevance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have your crawl_result\n",
    "\n",
    "\n",
    "def generate_batch_reports(crawl_result, batch_size=5):\n",
    "    \"\"\"\n",
    "    Process crawl results in batches and generate reports using an LLM.\n",
    "\n",
    "    Args:\n",
    "        crawl_result: The results from your web crawl\n",
    "        batch_size: Number of results to analyze in each LLM call\n",
    "\n",
    "    Returns:\n",
    "        A list of batch reports and a combined final report\n",
    "    \"\"\"\n",
    "    # Split crawl results into batches of 5\n",
    "    batches = []\n",
    "    for i in range(0, len(crawl_result), batch_size):\n",
    "        batch = crawl_result[i : i + batch_size]\n",
    "        batches.append(batch)\n",
    "\n",
    "    print(f\"Split crawl results into {len(batches)} batches\")\n",
    "\n",
    "    # Process each batch with the LLM\n",
    "    batch_reports = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"Processing batch {i+1}/{len(batches)}...\")\n",
    "\n",
    "        # Create a prompt for the LLM to analyze this batch\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following web crawl results and provide a detailed summary of the key information:\n",
    "        \n",
    "        {batch}\n",
    "                \n",
    "        Format your response as a well-structured report section.\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the LLM with the prompt\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        batch_reports.append(response)\n",
    "\n",
    "    # Combine the batch reports into a final comprehensive report\n",
    "    prompt = f\"\"\"\n",
    "    User Objective: {user_objective}\n",
    "    Create a report based on the findings:\n",
    "    {batch_reports}\n",
    "    \"\"\"\n",
    "\n",
    "    final_report = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return final_report.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function on your crawl results\n",
    "report_results = generate_batch_reports(crawl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save the report to a file\n",
    "with open(\"report.md\", \"w\") as f:\n",
    "    f.write(report_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the saved report file: [report.md](report.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL REPORT:\")\n",
    "print(report_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tavily-crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG System with Tavily Web Crawling\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to build a RAG system by crawling web content, processing it into chunks, and using it to answer questions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover the following steps:\n",
    "\n",
    "1. Crawl a website using Tavily's crawling API\n",
    "2. Extract and process the raw content\n",
    "3. Create documents with metadata\n",
    "4. Split documents into manageable chunks\n",
    "5. Create vector embeddings\n",
    "6. Build a question-answering system\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the Target Website\n",
    "\n",
    "We'll specify the base URL to crawl. For this example, we're using `tavily.com`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"tavily.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Crawl the Website\n",
    "\n",
    "Now we'll use Tavily's crawling API to extract content from the website. We can control the crawling behavior with parameters like:\n",
    "\n",
    "- `limit`: Maximum number of pages to crawl\n",
    "- `max_depth`: How many levels deep to crawl from the starting page\n",
    "- `max_breadth`: Maximum number of links to follow at each level\n",
    "- `extract_depth`: Level of content extraction (\"basic\" or \"advanced\")\n",
    "- `select_paths`: Specific URL paths to include\n",
    "- `select_domains`: Specific domains to include\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_result = requests.post(\n",
    "    \"https://api.tavily.com/crawl\",\n",
    "    headers={\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"},\n",
    "    json={\n",
    "        \"url\": base_url,\n",
    "        \"limit\": 100,\n",
    "        \"max_depth\": 3,\n",
    "        \"max_breadth\": 100,\n",
    "        \"extract_depth\": \"advanced\",\n",
    "        \"select_paths\": [\"/documentation/*\", \"/api-reference/*\"],\n",
    "        \"select_domains\": [\"docs\", \"blog\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examine Crawled URLs\n",
    "\n",
    "Let's look at the URLs that were successfully crawled. Note: the results will only be related to the documentation and api-reference path and the docs.tavily.com domain, as set in the `select_paths` and `select_domains` arguements.\n",
    "\n",
    "Hint: we can use these parameters to intelligently create vector databases...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in crawl_result.json()[\"results\"]:\n",
    "    print(page[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preview the Raw Content\n",
    "\n",
    "Let's examine a sample of the raw content from one of the crawled pages to understand what we're working with:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the data array from the JSON response\n",
    "data = crawl_result.json()[\"results\"]\n",
    "\n",
    "# Just view one sample page from the data array\n",
    "if data:\n",
    "    page = data[1]  # Get the first page\n",
    "    raw_content = page[\"raw_content\"]\n",
    "    print(f\"URL: {page['url']}\")\n",
    "    print(f\"Raw Content:{raw_content}...\")  # Print first 200 chars with ellipsis\n",
    "    print(\"-\" * 50)  # Print a separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Content into Documents\n",
    "\n",
    "We'll convert the crawled content into LangChain Document objects, which will allow us to:\n",
    "\n",
    "1. Maintain important metadata (source URL, page name)\n",
    "2. Prepare the text for chunking\n",
    "3. Make the content ready for vectorization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a list to store the Document objects\n",
    "documents = []\n",
    "\n",
    "# Loop through each page in your crawl results\n",
    "for page in crawl_result.json()[\"results\"]:\n",
    "    # Extract the content and URL/name for each page\n",
    "    page_content = page[\"raw_content\"]\n",
    "    page_url = page[\"url\"]\n",
    "\n",
    "    # Create a Document for each page with the URL and page name as metadata\n",
    "    doc = Document(\n",
    "        page_content=page_content,\n",
    "        metadata={\"source\": page_url, \"page_name\": page_url.split(\"/\")[-1]},\n",
    "    )\n",
    "\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Split Documents into Chunks\n",
    "\n",
    "We'll split the documents into smaller, more manageable chunks using the `RecursiveCharacterTextSplitter` and preview the result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you still want to split each page into smaller chunks:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunk size for page-level content\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "# Split the documents while preserving metadata\n",
    "all_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Now you have each page as a separate document with proper metadata\n",
    "print(f\"Created {len(documents)} page-level documents\")\n",
    "print(f\"Split into {len(all_chunks)} total chunks\")\n",
    "\n",
    "# Example of accessing the documents\n",
    "for i, doc in enumerate(documents[:2]):  # Print first 3 for example\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Page: {doc.metadata.get('page_name')}\")\n",
    "    print(f\"Source: {doc.metadata.get('source')}\")\n",
    "    print(f\"Content length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Vector Embeddings\n",
    "\n",
    "Now we'll create vector embeddings for our document chunks using OpenAI's embedding model and store them in a Chroma vector database. This allows us to perform semantic search on our document collection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a vector store from the loaded documents\n",
    "vector_store = Chroma.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build the Question-Answering System\n",
    "\n",
    "Finally, we'll create a retrieval-based question-answering system using gpt-4o-mini. We use the \"stuff\" chain type, which combines all relevant retrieved documents into a single context for the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "\n",
    "# Create a QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the System\n",
    "\n",
    "Let's test our RAG system by asking a question about Tavily's documentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "query = \"What is Tavily's production rate limit?\"\n",
    "answer = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully built a complete RAG system that can:\n",
    "\n",
    "1. Crawl web content from a specific domain\n",
    "2. Process and structure the content\n",
    "3. Create vector embeddings for semantic search\n",
    "4. Answer questions based on the crawled information\n",
    "\n",
    "This approach can be extended to create knowledge bases from any website, documentation, or content repository, making it a powerful tool for building domain-specific assistants and search systems. \n",
    "\n",
    "How you could enhance this by combining the Tavily `/Search` endpoint with the `/Crawl` endpoint 🤔... find out in the [Agentic Crawling Tutorial!](./agentic-crawl.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
